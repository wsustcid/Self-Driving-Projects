1
00:00:00,370 --> 00:00:04,269
To answer this question let's
look closely at the data.

2
00:00:04,269 --> 00:00:09,039
It seems that the red and green dots
are nicely separated by a line.

3
00:00:10,070 --> 00:00:14,140
Here is the line and we can see
that most points over it are green,

4
00:00:14,140 --> 00:00:17,859
whereas most points under it are red,
with some exceptions.

5
00:00:18,920 --> 00:00:23,760
So that line is going to be our model
and from now on, every time we get

6
00:00:23,760 --> 00:00:26,670
a new student, we check their scores and
plot them in this graph.

7
00:00:27,910 --> 00:00:30,899
If they end up over the line,
we predict that they'll get accepted.

8
00:00:31,989 --> 00:00:35,530
And if they end up below the line,
we predict that they'll get rejected.

9
00:00:35,530 --> 00:00:40,260
So, since student three ends up here and
the point with coordinate seven and

10
00:00:40,259 --> 00:00:44,689
six which is over the line, we conclude
that the string gets accepted.

11
00:00:46,500 --> 00:00:48,849
So, if you said yes,
that is the correct answer.

12
00:00:50,100 --> 00:00:53,420
This method is known as,
Logistic Regression.

13
00:00:53,420 --> 00:00:57,960
And now the question is, how do we find
this line that best cuts the data?

14
00:00:59,390 --> 00:01:02,460
So let's look at a simple example,
how do we draw the line that

15
00:01:02,460 --> 00:01:06,670
best separates the green
points from the red points?

16
00:01:06,670 --> 00:01:08,780
Well again the computer can't
really eyeball the line.

17
00:01:08,780 --> 00:01:11,379
And so, we can start by drawing
a random line like this one.

18
00:01:12,590 --> 00:01:15,900
And now given this line, let's
randomly say that we label the region

19
00:01:15,900 --> 00:01:19,530
over the line as green and
the region under the line as red.

20
00:01:20,831 --> 00:01:22,729
So just with linear regression,

21
00:01:22,730 --> 00:01:26,170
we'll first try to see how
bad this first line is.

22
00:01:26,170 --> 00:01:29,859
A simple measure of how bad the line is,
is just the number of errors.

23
00:01:29,859 --> 00:01:33,250
That is,
the number of misclassified points.

24
00:01:33,250 --> 00:01:38,319
This line misclassified two points,
one red and one green.

25
00:01:38,319 --> 00:01:40,059
So we'll say, it has two errors.

26
00:01:41,439 --> 00:01:43,109
And again with linear regression.

27
00:01:43,109 --> 00:01:47,209
What we'll do is, move the line around
trying to minimize the number of errors,

28
00:01:47,209 --> 00:01:48,419
using gradient descent.

29
00:01:49,640 --> 00:01:51,650
So if we move the line
a bit in this direction,

30
00:01:51,650 --> 00:01:55,530
we can see that we start correctly
classifying one of the points,

31
00:01:55,530 --> 00:01:57,620
bringing down the number
of errors to one.

32
00:01:58,790 --> 00:02:01,380
And if we move it even more,
we correctly classify the other one

33
00:02:01,379 --> 00:02:04,569
of the points, bringing down
the number of errors to zero.

34
00:02:06,150 --> 00:02:09,289
In reality, in order to properly
use the gradient descent method,

35
00:02:09,288 --> 00:02:12,759
it turns out the number of errors
is not what we need to minimize.

36
00:02:12,759 --> 00:02:13,840
But instead,

37
00:02:13,840 --> 00:02:18,420
something that captures the number
of errors called log loss function.

38
00:02:18,419 --> 00:02:20,699
Let's actually study
that in more detail.

39
00:02:20,699 --> 00:02:23,939
So here are our six points again with
four of them correctly classified,

40
00:02:23,939 --> 00:02:25,669
two red and two green and

41
00:02:25,669 --> 00:02:28,609
two of them incorrectly classified,
one red and one green.

42
00:02:29,860 --> 00:02:33,410
The error function will assign
a large penalty to the two incorrectly

43
00:02:33,409 --> 00:02:39,039
classified points and small penalty to
the four correctly classified points.

44
00:02:39,039 --> 00:02:41,799
We'll actually learn the formula for
the error function in the class.

45
00:02:43,099 --> 00:02:46,030
Now we obtain the error function
by adding all the errors

46
00:02:46,030 --> 00:02:49,140
from the corresponding points,
here we obtain a large

47
00:02:49,139 --> 00:02:52,799
number since the two misclassified
points add a lot to the error function.

48
00:02:53,939 --> 00:02:57,800
Now the idea is to, jiggle the line
around to minimize this error.

49
00:02:59,099 --> 00:03:00,849
So if we move the line
in this direction,

50
00:03:00,849 --> 00:03:05,509
we can see that some errors decrease and
some slightly increase.

51
00:03:05,509 --> 00:03:09,159
But in general, when we consider the
sum, the sum gets smaller since we've

52
00:03:09,159 --> 00:03:12,799
correctly classified the two points
that were misclassified before.

53
00:03:13,939 --> 00:03:15,969
So the point of this process is,

54
00:03:15,969 --> 00:03:19,759
to find the best line fit by
minimizing the error function.

55
00:03:21,139 --> 00:03:23,399
And how do we minimize
the error function?

56
00:03:23,400 --> 00:03:24,870
Again, with gradient descent.

57
00:03:26,129 --> 00:03:28,944
So once more, here we are at
the summit of Mount Errorest,

58
00:03:28,944 --> 00:03:33,120
we're quite high up, because our
error is large, we're you can see,

59
00:03:33,120 --> 00:03:36,819
how the height is the sum
of the green and red areas.

60
00:03:36,819 --> 00:03:40,000
So, we explore around to see what
direction brings us down the most or

61
00:03:40,000 --> 00:03:41,030
equivalently.

62
00:03:41,030 --> 00:03:46,219
What direction can we move the line in
order to reduce the error the most?

63
00:03:46,219 --> 00:03:48,310
And we decide to take
a step in that direction.

64
00:03:48,310 --> 00:03:51,460
So now, we're misclassifying
only one of the points.

65
00:03:51,460 --> 00:03:55,270
We can see how that decreases error
function, bringing us down the mountain.

66
00:03:55,270 --> 00:03:56,180
And we do it again,

67
00:03:56,180 --> 00:04:00,300
we take a step in the direction
that reduces the error the most.

68
00:04:00,300 --> 00:04:01,820
Now, we're at the bottom
of the mountain,

69
00:04:01,819 --> 00:04:04,729
since we've reduced the error
to its minimum, possible value.

