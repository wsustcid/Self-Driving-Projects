1
00:00:00,330 --> 00:00:03,010
Okay, so now we have all
the pieces of our puzzle.

2
00:00:03,010 --> 00:00:07,080
The question of course is how we're
going to find those weights w and

3
00:00:07,080 --> 00:00:11,880
those biases b that will get our
classifier to do what we want it to do.

4
00:00:11,880 --> 00:00:15,760
That is, have a low distance for
the correct class but

5
00:00:15,760 --> 00:00:19,400
have a high distance for
the incorrect class.

6
00:00:19,400 --> 00:00:23,290
One thing you can do is measure that
distance averaged over the entire

7
00:00:23,290 --> 00:00:28,340
training sets for all the inputs and
all the labels that you have available.

8
00:00:28,340 --> 00:00:30,240
That's called the training loss.

9
00:00:30,240 --> 00:00:33,840
This loss, which is the average
cross-entropy over your entire training

10
00:00:33,840 --> 00:00:37,140
set, Is one humongous function.

11
00:00:37,140 --> 00:00:40,800
Every example in your training set gets
multiplied by this one big matrix W.

12
00:00:40,800 --> 00:00:44,190
And then they get all
added up in one big sum.

13
00:00:45,240 --> 00:00:48,020
We want all the distances to be small,
which would mean we're

14
00:00:48,020 --> 00:00:51,600
doing a good job at classifying
every example in the training data.

15
00:00:51,600 --> 00:00:53,130
So we want the loss to be small.

16
00:00:54,220 --> 00:00:56,890
The loss is a function of
the weights and the biases.

17
00:00:56,890 --> 00:00:59,140
So we are simply going to try and
minimize that function.

18
00:01:00,160 --> 00:01:02,730
Imagine that the loss is
a function of two weights.

19
00:01:02,730 --> 00:01:04,560
Weight one and weight two.

20
00:01:04,560 --> 00:01:05,970
Just for the sake of argument.

21
00:01:05,970 --> 00:01:09,670
It's going to be a function which
will be large in some areas, and

22
00:01:09,670 --> 00:01:11,200
small in others.

23
00:01:11,200 --> 00:01:15,540
We're going to try the weights which
cause this loss to be the smallest.

24
00:01:15,540 --> 00:01:18,210
We've just turned
the machine learning problem

25
00:01:18,210 --> 00:01:20,770
into one of numerical optimization.

26
00:01:20,770 --> 00:01:24,390
And there's lots of ways to solve
a numerical optimization problem.

27
00:01:24,390 --> 00:01:29,060
The simplest way is one you've probably
encountered before, gradient descent.

28
00:01:29,060 --> 00:01:32,260
Take the derivative of your loss,
with respect to your parameters, and

29
00:01:32,260 --> 00:01:35,635
follow that derivative by
taking a step backwards and

30
00:01:35,635 --> 00:01:37,890
repeat until you get to the bottom.

31
00:01:37,890 --> 00:01:40,760
Gradient descent is relatively simple,
especially when you

32
00:01:40,760 --> 00:01:44,310
have powerful numerical tools that
compute the derivatives for you.

33
00:01:44,310 --> 00:01:46,370
Remember, I'm showing
you the derivative for

34
00:01:46,370 --> 00:01:50,630
a function of just two parameters here,
but for a typical problem

35
00:01:50,630 --> 00:01:54,350
it could be a function of thousands,
millions or even billions of parameters.

