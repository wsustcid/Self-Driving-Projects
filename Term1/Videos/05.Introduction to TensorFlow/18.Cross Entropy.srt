1
00:00:00,002 --> 00:00:03,961
When hot encoding thing including works
very well from most problems until you

2
00:00:03,961 --> 00:00:06,842
get into situations where you
have tens of thousands, or

3
00:00:06,842 --> 00:00:08,224
even millions of classes.

4
00:00:08,224 --> 00:00:11,641
In that case, your vector
becomes really, really large and

5
00:00:11,641 --> 00:00:15,890
has mostly zeros everywhere and
that becomes very inefficient.

6
00:00:15,890 --> 00:00:19,840
You'll see later how we can deal with
these problems using embeddings.

7
00:00:19,840 --> 00:00:24,180
What's nice about this approach is that
we can now measure how well we're doing

8
00:00:24,180 --> 00:00:26,220
by simply comparing two vectors.

9
00:00:26,220 --> 00:00:29,430
One that comes out of your classifiers
and contains the probabilities of your

10
00:00:29,430 --> 00:00:34,070
classes and the one hot encoded vector
that corresponds to your labels.

11
00:00:34,070 --> 00:00:36,030
Let's see how we can
do this in practice.

12
00:00:36,030 --> 00:00:40,450
The natural way to measure the distance
between those two probability vectors is

13
00:00:40,450 --> 00:00:42,079
called the cross-entropy.

14
00:00:42,079 --> 00:00:44,795
I'll denote it by D here for distance.

15
00:00:44,795 --> 00:00:47,354
Math, it looks like this.

16
00:00:47,354 --> 00:00:50,067
Be careful,
the cross-entropy is not symmetric and

17
00:00:50,067 --> 00:00:51,582
you have a nasty log in there.

18
00:00:51,582 --> 00:00:54,011
So you have to make sure
that your labels and

19
00:00:54,011 --> 00:00:56,516
your distributions
are in the right place.

20
00:00:56,516 --> 00:01:00,212
Your labels, because they're one
hot encoded, will have a lot of

21
00:01:00,212 --> 00:01:03,540
zeroes in them and you don't
want to take the log of zeroes.

22
00:01:03,540 --> 00:01:07,350
For your distribution, the softmax will
always guarantee that you have a little

23
00:01:07,350 --> 00:01:11,650
bit of probability going everywhere, so
you never really take a log of zero.

24
00:01:11,650 --> 00:01:14,490
So lets recap,
because we have a lot of pieces already.

25
00:01:14,490 --> 00:01:19,290
So we have an input, it's going to be
turned into logits using a linear model,

26
00:01:19,290 --> 00:01:22,800
which is basically your
matrix multiply and a bias.

27
00:01:22,800 --> 00:01:26,244
We're then going to feed the logits,
which are scores,

28
00:01:26,244 --> 00:01:29,205
into a softmax to turn
them into probabilities.

29
00:01:29,205 --> 00:01:32,803
And then we're going to compare those
probabilities to the one hot encoded

30
00:01:32,803 --> 00:01:35,330
labels using the cross entropy function.

31
00:01:35,330 --> 00:01:39,310
This entire setting is often called
multinomial logistic classification.
