1
00:00:00,410 --> 00:00:03,330
There's another important technique for
regularization

2
00:00:03,330 --> 00:00:06,759
that only emerged relatively
recently and works amazingly well.

3
00:00:08,028 --> 00:00:12,230
It also looks insane the first
time you see it, so bear with me.

4
00:00:12,230 --> 00:00:13,663
It's called Dropout.

5
00:00:13,663 --> 00:00:15,859
Dropout works like this.

6
00:00:15,859 --> 00:00:19,339
Imagine that you have one layer
that connects to another layer.

7
00:00:19,339 --> 00:00:23,140
The values that go from one layer to
the next are often called activations.

8
00:00:24,399 --> 00:00:26,209
Now take those activations and

9
00:00:26,210 --> 00:00:31,015
randomly, for every example you train
your network on, set half of them to 0.

10
00:00:32,310 --> 00:00:36,250
Completely and randomly, you basically
take half of the data that's flowing

11
00:00:36,250 --> 00:00:40,159
through your network, and
just destroy it and then randomly again.

12
00:00:41,390 --> 00:00:44,899
If that does not sound crazy to you,
then you might qualify to become

13
00:00:44,899 --> 00:00:48,119
a student of Geoffrey Hinton,
who pioneered the technique.

14
00:00:48,119 --> 00:00:50,070
So what happens with dropout?

15
00:00:50,070 --> 00:00:55,329
Your network can never rely on any
given activation to be present,

16
00:00:55,329 --> 00:00:57,820
because they might be
squashed at any given moment.

17
00:00:58,890 --> 00:01:01,810
So it is forced to learn
a redundant representation for

18
00:01:01,810 --> 00:01:06,150
everything to make sure that at least
some of the information remains.

19
00:01:06,150 --> 00:01:08,170
It's like a game of whack-a-mole.

20
00:01:08,170 --> 00:01:11,409
One activations get smashed,
but there is always one or

21
00:01:11,409 --> 00:01:13,840
more that do the same job.

22
00:01:13,840 --> 00:01:15,340
And that don't get killed.

23
00:01:15,340 --> 00:01:18,120
So, everything remains fine at the end.

24
00:01:18,120 --> 00:01:21,770
Forcing your network to learn redundant
representations might sound very

25
00:01:21,769 --> 00:01:23,000
inefficient.

26
00:01:23,000 --> 00:01:27,230
But in practice, it makes things more
robust, and prevents over fitting.

27
00:01:27,230 --> 00:01:31,670
It also makes your network act as if
taking the consensus over an ensemble of

28
00:01:31,670 --> 00:01:35,105
networks, which is always a good
way to improve performance.

29
00:01:35,105 --> 00:01:38,510
Dropout is one of the most important
techniques to emerge in the last few

30
00:01:38,510 --> 00:01:39,252
years.

31
00:01:39,251 --> 00:01:40,399
If Dropout doesn't work for

32
00:01:40,400 --> 00:01:43,020
you, you should probably
be using a bigger network.

