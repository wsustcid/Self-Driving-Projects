1
00:00:00,490 --> 00:00:03,330
>> When you evaluate the network
that's been trained with drop out,

2
00:00:03,330 --> 00:00:06,220
you obviously no longer
want this randomness.

3
00:00:06,220 --> 00:00:08,270
You want something deterministic.

4
00:00:08,270 --> 00:00:10,940
Instead, you're going to
want to take the consensus

5
00:00:10,940 --> 00:00:13,190
over these redundant models.

6
00:00:13,190 --> 00:00:16,826
You get the consensus opinion
by averaging the activations.

7
00:00:16,826 --> 00:00:19,990
You want Y e here to be the average

8
00:00:19,990 --> 00:00:23,130
of all the yts that you
got during training.

9
00:00:23,130 --> 00:00:26,540
Here's a trick to make sure
this expectation holds.

10
00:00:26,540 --> 00:00:30,260
During training, not only do you use
zero out so the activations that you

11
00:00:30,260 --> 00:00:35,470
drop out, but you also scale the
remaining activations by a factor of 2.

12
00:00:35,470 --> 00:00:39,300
This way, when it comes time to
average them during evaluation,

13
00:00:39,300 --> 00:00:43,150
you just remove these dropouts and
scaling operations from your neural net.

14
00:00:43,150 --> 00:00:47,214
And the result is an average of these
activations that is properly scaled.

