1
00:00:00,000 --> 00:00:03,260
Here, we set up our TensorFlow variables.

2
00:00:03,260 --> 00:00:06,810
x is a placeholder that will store our input batches.

3
00:00:06,810 --> 00:00:09,690
We initialize the batch size to None,

4
00:00:09,690 --> 00:00:13,415
which allows the placeholder to later accept a batch of any size,

5
00:00:13,415 --> 00:00:19,780
and we set the image dimensions to 32 by 32 by 1. y stores our labels.

6
00:00:19,780 --> 00:00:23,310
In this case, our labels come through with sparse variables,

7
00:00:23,310 --> 00:00:25,365
which just means that they're integers.

8
00:00:25,365 --> 00:00:27,315
They aren't one-hot encoded yet.

9
00:00:27,315 --> 00:00:31,770
We use the tf.one_hot function to one-hot encode the labels.

10
00:00:31,770 --> 00:00:34,440
Now we set up our training pipeline.

11
00:00:34,440 --> 00:00:37,070
First, we have another hyperparameter to tune.

12
00:00:37,070 --> 00:00:41,605
The learning rate tells TensorFlow how quickly to update the network's weights.

13
00:00:41,605 --> 00:00:44,810
0.001 is a good default value

14
00:00:44,810 --> 00:00:47,595
but you can experiment with other rates and see how they do.

15
00:00:47,595 --> 00:00:52,795
Next, we pass the input data to the LeNet function to calculate our logits.

16
00:00:52,795 --> 00:00:57,170
We used the tf.nn.softmax_cross_entropy_with_logits

17
00:00:57,170 --> 00:00:59,450
function to compare those logits to

18
00:00:59,450 --> 00:01:02,715
the ground truth labels and calculate the cross entropy.

19
00:01:02,715 --> 00:01:05,480
Cross entropy is just a measure of how different

20
00:01:05,480 --> 00:01:08,360
the logits are from the ground truth training labels.

21
00:01:08,360 --> 00:01:13,925
The tf.reduce_mean function averages the cross entropy from all of the training images.

22
00:01:13,925 --> 00:01:17,810
AdamOptimizer uses the Adam algorithm to minimize

23
00:01:17,810 --> 00:01:22,480
the loss function similarly to what stochastic gradient descent does.

24
00:01:22,480 --> 00:01:27,345
The Adam algorithm is a little more sophisticated than stochastic gradient descent,

25
00:01:27,345 --> 00:01:29,905
so it's a good default choice for an optimizer.

26
00:01:29,905 --> 00:01:33,780
This is where we use the learning rate hyperparameter that we set earlier.

27
00:01:33,780 --> 00:01:37,410
Finally, we run the minimize function on the optimizer which

28
00:01:37,410 --> 00:01:41,960
uses backpropagation to update the network and minimize our training loss.

29
00:01:41,960 --> 00:01:43,760
So this is our training pipeline.

30
00:01:43,760 --> 00:01:46,527
But so far, it's only a pipeline,

31
00:01:46,527 --> 00:01:50,010
we have to actually pass data into the pipeline for it to work.

32
00:01:50,010 --> 00:01:52,000
We'll get to that in a few code cells.
